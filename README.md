# Stock Pipeline: Airflow(Local) + Spark(Local) + GCP for Data Engineering

## Project Overview
This project demonstrates a complete Data Engineering Pipeline for processing historical stock and fundamental data.
It incorporates Airflow for orchestration, PySpark for data processing, and integrates with Google Cloud Platform (GCP) services such as Google Cloud Storage (GCS) and BigQuery.
The orchestration and data processing components (Airflow and Spark) are run locally using Docker.
